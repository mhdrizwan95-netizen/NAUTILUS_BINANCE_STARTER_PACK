{
  "findings": [
    {
      "id": "frontend-fetch-no-timeout",
      "severity": "S2",
      "area": "network",
      "file": "frontend/src/lib/api.ts",
      "lines": "41-69",
      "evidence": "`api()` awaited `fetch` without any timeout or abort choreography, so stalled backends left the dashboard spinning indefinitely.",
      "impact": "Browser fetch defaults wait forever; when ops API hangs the React Query callers never recover, pinning spinners and blocking follow-up retries across the UI.",
      "fix": "Wrap fetch calls with an AbortController-based timeout helper, propagate HTTP status codes, and surface a clear timeout error after 10s.",
      "diff": "diff --git a/frontend/src/lib/api.ts b/frontend/src/lib/api.ts\nindex 60c3db2..9cdd9d4 100644\n--- a/frontend/src/lib/api.ts\n+++ b/frontend/src/lib/api.ts\n@@\n-async function api<T>(path: string, init?: RequestInit, signal?: AbortSignal): Promise<T> {\n-  const response = await fetch(`${BASE}${path}`, {\n-    ...init,\n-    headers: {\n-      'Content-Type': 'application/json',\n-      ...(init?.headers ?? {}),\n-    },\n-    signal,\n-  });\n-\n-  if (!response.ok) {\n-    throw new Error(await response.text());\n-  }\n-\n-  return response.json() as Promise<T>;\n-}\n+const DEFAULT_TIMEOUT_MS = 10_000;\n+\n+interface TimeoutSignal {\n+  signal: AbortSignal;\n+  cleanup: () => void;\n+}\n+\n+const createTimeoutSignal = (signal?: AbortSignal, timeoutMs: number = DEFAULT_TIMEOUT_MS): TimeoutSignal => {\n+  if (signal?.aborted) {\n+    return { signal, cleanup: () => {} };\n+  }\n+\n+  const controller = new AbortController();\n+  const timeoutId = window.setTimeout(() => {\n+    controller.abort(new DOMException(`Request timed out after ${timeoutMs}ms`, 'TimeoutError'));\n+  }, timeoutMs);\n+\n+  let relayAbort: (() => void) | undefined;\n+\n+  if (signal) {\n+    relayAbort = () => {\n+      controller.abort(signal.reason);\n+    };\n+    signal.addEventListener('abort', relayAbort, { once: true });\n+  }\n+\n+  const cleanup = () => {\n+    window.clearTimeout(timeoutId);\n+    if (signal && relayAbort) {\n+      signal.removeEventListener('abort', relayAbort);\n+    }\n+  };\n+\n+  return { signal: controller.signal, cleanup };\n+};\n+\n+async function api<T>(path: string, init?: RequestInit, signal?: AbortSignal): Promise<T> {\n+  const { signal: timeoutSignal, cleanup } = createTimeoutSignal(signal);\n+\n+  try {\n+    const response = await fetch(`${BASE}${path}`, {\n+      ...init,\n+      headers: {\n+        'Content-Type': 'application/json',\n+        ...(init?.headers ?? {}),\n+      },\n+      signal: timeoutSignal,\n+    });\n+\n+    if (!response.ok) {\n+      const body = await response.text();\n+      const error = new Error(body || `Request to ${path} failed with ${response.status}`);\n+      (error as Error & { status?: number }).status = response.status;\n+      throw error;\n+    }\n+\n+    return response.json() as Promise<T>;\n+  } catch (error) {\n+    if (error instanceof DOMException && error.name == 'TimeoutError') {\n+      throw new Error(`Request to ${path} timed out after ${DEFAULT_TIMEOUT_MS}ms`);\n+    }\n+    throw error;\n+  } finally {\n+    cleanup();\n+  }\n+}"
    },
    {
      "id": "executor-httpx-defaults",
      "severity": "S2",
      "area": "network",
      "file": "ops/main.py",
      "lines": "80-154",
      "evidence": "Executor heartbeat and probe helpers instantiated bare `httpx.AsyncClient()` instances, so connect/pool timeouts were implicit and proxies/CA bundles from the environment were ignored.",
      "impact": "Under engine stalls or corporate proxy requirements the executor would fan out up to 100 default connections, hold sockets open for 5s per call, and drop all traffic when the runtime needed the system proxy chain, resulting in flapping health metrics and failed trades during outages.",
      "fix": "Bind connect/read/pool timeouts, shrink the connection pool, enable `trust_env=True`, and mirror those defaults in the ad-hoc price fetch fallback. Document the limits in `net_checklist.md` so operators keep the guardrails.",
      "diff": "diff --git a/ops/main.py b/ops/main.py\nindex 744c2b5..8f341a9 100644\n--- a/ops/main.py\n+++ b/ops/main.py\n@@\n-    async with httpx.AsyncClient() as client:\n+    client_limits = httpx.Limits(max_connections=8, max_keepalive_connections=4)\n+    client_timeout = httpx.Timeout(connect=3.0, read=5.0, write=5.0, pool=5.0)\n+\n+    async with httpx.AsyncClient(\n+        timeout=client_timeout, limits=client_limits, trust_env=True\n+    ) as client:\n@@\n-    try:\n-        async with httpx.AsyncClient(timeout=timeout, limits=limits) as client:\n+    try:\n+        async with httpx.AsyncClient(\n+            timeout=timeout, limits=limits, trust_env=True\n+        ) as client:\n             for r in range(99999):  # Infinite rounds, will be interrupted\n                 # Step 1: Fetch ALL prices in one round-trip (O(1) vs O(symbols))\n                 price_map = await fetch_all_prices(ENGINE_URL, client)\ndiff --git a/ops/auto_probe.py b/ops/auto_probe.py\nindex 4932db9..2c70b4d 100644\n--- a/ops/auto_probe.py\n+++ b/ops/auto_probe.py\n@@\n-        if temp_client:\n-            async with httpx.AsyncClient(timeout=4.0) as client:\n+        if temp_client:\n+            async with httpx.AsyncClient(\n+                timeout=httpx.Timeout(connect=2.0, read=4.0, write=4.0, pool=4.0),\n+                limits=httpx.Limits(max_connections=4, max_keepalive_connections=4),\n+                trust_env=True,\n+            ) as client:\n             r = await client.get(f\"{engine_base}/prices\")\n             r.raise_for_status()\n             data = r.json().get(\"prices\", {})\ndiff --git a/net_checklist.md b/net_checklist.md\nindex eb23b71..7da66dc 100644\n--- a/net_checklist.md\n+++ b/net_checklist.md\n@@\n-  - Engine venue clients: httpx timeouts from env (`BINANCE_API_TIMEOUT` seconds). Backoff on 418/429 implemented.\n-  - Ops/ML/Ingester endpoints: set short, explicit timeouts; avoid relying on defaults.\n+  - Engine venue clients: httpx timeouts from env (`BINANCE_API_TIMEOUT` seconds). Backoff on 418/429 implemented.\n+  - Ops/ML/Ingester endpoints: set short, explicit timeouts; avoid relying on defaults.\n+  - Frontend fetches: Abort after 10s via shared timeout helper to avoid hung UI.\n@@\n - Keep-Alive\n   - Prefer long-lived AsyncClients in loops; avoid recreate-per-request in hot paths to reduce handshake overhead.\n+  - Executor heartbeat uses pooled httpx client (`Limits(8/4)`) and respects proxies via `trust_env=True`."
    }
  ],
  "risk_rollup": {
    "S0": 0,
    "S1": 0,
    "S2": 2,
    "S3": 0
  },
  "tickets": [
    {
      "title": "Document and surface new client timeout defaults in operator runbooks",
      "owner": "operations",
      "eta": "2d",
      "blocks": [],
      "acceptance": [
        "Runbook links to net_checklist.md",
        "Operators update on-call dashboard with 10s UI timeout guidance",
        "Executor deployment verifies new timeout env var expectations"
      ]
    }
  ],
  "state": {
    "network_defaults": {
      "frontend_timeout_ms": 10000,
      "executor_httpx_limits": "connect=3s/read=5s/pool=5s limits(8,4)",
      "auto_probe_fallback_limits": "connect=2s/read=4s limits(4,4)"
    },
    "net_docs": [
      "net_checklist.md"
    ],
    "external_endpoints": [
      "engine_binance",
      "ops"
    ]
  }
}
